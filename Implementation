1) *Importing the required libraries*
After analyzing the data our next step is to import the required libraries for our project. Some of the libraries we use in this project are pandas, numpy, scikit learn, and nltk.After analyzing the data our next step is to import the required libraries for our project. Some of the libraries we use in this project are pandas, numpy, scikit learn, and nltk.
2) *Preprocessing the data*
In Data preprocessing, we prepare the raw data and make it suitable for a machine learning model. It is the first and crucial step while creating a machine learning model. When creating a machine learning project, it is not always a case that we come across clean and formatted data. And while doing any operation with data, it is mandatory to clean it and put it in a formatted way. So for this, we use the data preprocessing task.
3) *Splitting the data*
Splitting the data into two parts, training and testing, is super important for making machine learning models. The first group, called training data, is used to teach the model. It's like giving examples for the model to learn from. The second group, called testing data, is kept aside and not used for teaching. Instead, we use it to check how good the model is at making predictions on new data it hasn't seen before. This helps us figure out if the model is working well or if it's making mistakes that we need to fix.
4) *Building the model*
Once we've divided our data into training and testing sets, the next step is to select a suitable algorithm for our model. for this purpose BERT(Bidirectional Encoder Representations from Transformers) is used,which is a powerful tool in the realm of Natural Language Processing (NLP). It's essentially a pre-trained language model that has been trained on a massive dataset of text and code, allowing it to understand the nuances of human language in a remarkable way.
5) *Evaluating the results*
The final step in building a machine learning model is the prediction phase, where we evaluate how well our trained model performs on new, unseen data. This step is crucial for assessing the model's effectiveness and determining its real-world applicability. 
